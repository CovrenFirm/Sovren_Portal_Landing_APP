###############################################################################
# Sovren Prometheus Alert Rules
# Production-grade alerting for Portal + H100 Cluster
###############################################################################

groups:
  - name: service_health
    interval: 10s
    rules:
      - alert: ServiceDown
        expr: up == 0
        for: 30s
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "{{ $labels.job }} on {{ $labels.node }} has been down for more than 30 seconds."

      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[1m]) > 0.05
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "High 5xx error rate on {{ $labels.service }}"
          description: "{{ $labels.service }} is experiencing {{ $value }} errors per second."

  - name: wireguard_health
    interval: 10s
    rules:
      - alert: WireGuardHighLatency
        expr: probe_duration_seconds{job="wireguard-icmp"} > 0.030
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "High WireGuard latency to {{ $labels.node }}"
          description: "ICMP RTT to {{ $labels.instance }} ({{ $labels.node }}) is {{ $value }}s (>30ms threshold)."

      - alert: WireGuardNodeUnreachable
        expr: probe_success{job="wireguard-icmp"} == 0
        for: 30s
        labels:
          severity: critical
        annotations:
          summary: "WireGuard node {{ $labels.node }} unreachable"
          description: "ICMP probe to {{ $labels.instance }} ({{ $labels.node }}) is failing."

  - name: crm_backend
    interval: 10s
    rules:
      - alert: CRMHighContactCreationLatency
        expr: histogram_quantile(0.95, rate(crm_contact_creation_duration_seconds_bucket[5m])) > 1.0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "CRM contact creation latency high"
          description: "P95 contact creation latency is {{ $value }}s (>1s threshold)."

      - alert: ShadowBoardAnalysisFailures
        expr: rate(shadowboard_analysis_failures_total[5m]) > 0.01
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Shadow Board analysis failures detected"
          description: "Shadow Board is experiencing {{ $value }} analysis failures per second."

  - name: voice_pipeline
    interval: 10s
    rules:
      - alert: TTSHighLatency
        expr: histogram_quantile(0.95, rate(tts_synthesis_duration_seconds_bucket[5m])) > 2.0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "TTS synthesis latency high"
          description: "P95 TTS latency is {{ $value }}s (>2s threshold)."

      - alert: VoilaDuplexDown
        expr: up{job="voila-duplex"} == 0
        for: 30s
        labels:
          severity: critical
        annotations:
          summary: "Voila Duplex service down"
          description: "Voila Duplex on g374 (10.15.38.102:8700) has been down for 30+ seconds."

  - name: prometheus_internal
    interval: 30s
    rules:
      - alert: PrometheusTargetDown
        expr: up{job="prometheus"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Prometheus is down"
          description: "Prometheus self-monitoring shows the service is down."

      - alert: PrometheusScrapeFailures
        expr: rate(prometheus_target_scrapes_exceeded_sample_limit_total[5m]) > 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus scrape failures"
          description: "Prometheus is failing to scrape {{ $labels.job }}."
